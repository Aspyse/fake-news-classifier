{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fake-news.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "l8LnAF1UFuKr"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15UIaXcX3kdp"
      },
      "source": [
        "***IMPORTANT: COLAB AUTH CODE HERE***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGPie9sL3yJw",
        "outputId": "e62cd879-83eb-4662-c198-671448ffa343"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZqtjVaNG1v2"
      },
      "source": [
        "# ***processing components***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpdsDHLPFMRD",
        "outputId": "82e96564-b2ca-46df-9c74-bc8e6a1ee77f"
      },
      "source": [
        "!pip install kaggle\n",
        "import json\n",
        "\n",
        "# auth API using kaggle.json\n",
        "with open('drive/MyDrive/kaggle.json') as json_file:\n",
        "    uploaded = json.load(json_file)\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "!mkdir -p ~/.kaggle/ && cp drive/MyDrive/kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.10)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2020.12.5)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "User uploaded file \"username\" with length 6 bytes\n",
            "User uploaded file \"key\" with length 32 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Lp7azdLCEuE",
        "outputId": "59583d7c-8812-4f21-88bc-a30355ea3678"
      },
      "source": [
        "# downloading dataset using API\n",
        "\n",
        "!kaggle datasets download -d clmentbisaillon/fake-and-real-news-dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading fake-and-real-news-dataset.zip to /content\n",
            " 66% 27.0M/41.0M [00:00<00:00, 33.0MB/s]\n",
            "100% 41.0M/41.0M [00:00<00:00, 69.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubLgjhHIJ8uq",
        "outputId": "9c536a2e-4178-43b0-f636-0ce41a996579"
      },
      "source": [
        "!ls datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access 'datasets': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gP-ROkQur9m"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"fake-and-real-news-dataset.zip\",\"r\") as zip_ref:\n",
        "  zip_ref.extractall(\"datasets\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NL_XJbKOtGf",
        "outputId": "e3b675b6-b339-42f9-8a69-26678e2092b0"
      },
      "source": [
        "!git clone https://github.com/facebookresearch/fastText.git\n",
        "%cd fastText\n",
        "!sudo pip install .\n",
        "%cd ..\n",
        "\n",
        "import fasttext\n",
        "\n",
        "# downloading pre-trained vectors (wikipedia)\n",
        "# fasttext.util.download_model('en', if_exists='ignore')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fastText'...\n",
            "remote: Enumerating objects: 3854, done.\u001b[K\n",
            "remote: Total 3854 (delta 0), reused 0 (delta 0), pack-reused 3854\u001b[K\n",
            "Receiving objects: 100% (3854/3854), 8.23 MiB | 32.15 MiB/s, done.\n",
            "Resolving deltas: 100% (2416/2416), done.\n",
            "/content/fastText\n",
            "Processing /content/fastText\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (2.6.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (54.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3090553 sha256=f69ae59414fabe29a449a229431e6f89e68fdeea2ad415db07be3ab95acf3d83\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vub3_1pi/wheels/a1/9f/52/696ce6c5c46325e840c76614ee5051458c0df10306987e7443\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MLI9px3Fi7O"
      },
      "source": [
        "# ***training main***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8LnAF1UFuKr"
      },
      "source": [
        "## loading vectors (not used)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcfsegYBNfze"
      },
      "source": [
        "# ft = fasttext.load_model('cc.en.300.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nnj7-R2U5tfd"
      },
      "source": [
        "# ft.get_nearest_neighbors('arrow')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9QA8VkKGyV0"
      },
      "source": [
        "## text preprocessing functions\n",
        "*modified from Charles Malafosse's code*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M712JFbQHEN3"
      },
      "source": [
        "# taken from FastText sentiment analysis for text by Charles Malafosse\n",
        "import re\n",
        "import itertools\n",
        " \n",
        "def load_dict_contractions():\n",
        "    return {\n",
        "        \"aren't\":\"are not\",\n",
        "        \"can't\":\"cannot\",\n",
        "        \"couldn't\":\"could not\",\n",
        "        \"could've\":\"could have\",\n",
        "        \"didn't\":\"did not\",\n",
        "        \"doesn't\":\"does not\",\n",
        "        \"don't\":\"do not\",\n",
        "        \"everyone's\":\"everyone is\",\n",
        "        \"gimme\":\"give me\",\n",
        "        \"gonna\":\"going to\",\n",
        "        \"gon't\":\"go not\",\n",
        "        \"gotta\":\"got to\",\n",
        "        \"hadn't\":\"had not\",\n",
        "        \"hasn't\":\"has not\",\n",
        "        \"haven't\":\"have not\",\n",
        "        \"he'd\":\"he would\",\n",
        "        \"he'll\":\"he will\",\n",
        "        \"he's\":\"he is\",\n",
        "        \"he've\":\"he have\",\n",
        "        \"how'd\":\"how would\",\n",
        "        \"how'll\":\"how will\",\n",
        "        \"how're\":\"how are\",\n",
        "        \"how's\":\"how is\",\n",
        "        \"I'd\":\"I would\",\n",
        "        \"I'll\":\"I will\",\n",
        "        \"I'm\":\"I am\",\n",
        "        \"isn't\":\"is not\",\n",
        "        \"it'd\":\"it would\",\n",
        "        \"it'll\":\"it will\",\n",
        "        \"it's\":\"it is\",\n",
        "        \"I've\":\"I have\",\n",
        "        \"kinda\":\"kind of\",\n",
        "        \"let's\":\"let us\",\n",
        "        \"might've\":\"might have\",\n",
        "        \"mustn't\":\"must not\",\n",
        "        \"must've\":\"must have\",\n",
        "        \"she'd\":\"she would\",\n",
        "        \"she'll\":\"she will\",\n",
        "        \"she's\":\"she is\",\n",
        "        \"shouldn't\":\"should not\",\n",
        "        \"should've\":\"should have\",\n",
        "        \"somebody's\":\"somebody is\",\n",
        "        \"someone's\":\"someone is\",\n",
        "        \"something's\":\"something is\",\n",
        "        \"that'd\":\"that would\",\n",
        "        \"that'll\":\"that will\",\n",
        "        \"that're\":\"that are\",\n",
        "        \"that's\":\"that is\",\n",
        "        \"there'd\":\"there would\",\n",
        "        \"there'll\":\"there will\",\n",
        "        \"there're\":\"there are\",\n",
        "        \"there's\":\"there is\",\n",
        "        \"they'd\":\"they would\",\n",
        "        \"they'll\":\"they will\",\n",
        "        \"they're\":\"they are\",\n",
        "        \"they've\":\"they have\",\n",
        "        \"those're\":\"those are\",\n",
        "        \"wanna\":\"want to\",\n",
        "        \"wasn't\":\"was not\",\n",
        "        \"we'd\":\"we would\",\n",
        "        \"we'll\":\"we will\",\n",
        "        \"we're\":\"we are\",\n",
        "        \"weren't\":\"were not\",\n",
        "        \"we've\":\"we have\",\n",
        "        \"what'd\":\"what did\",\n",
        "        \"what'll\":\"what will\",\n",
        "        \"what're\":\"what are\",\n",
        "        \"what's\":\"what is\",\n",
        "        \"what've\":\"what have\",\n",
        "        \"when's\":\"when is\",\n",
        "        \"where'd\":\"where did\",\n",
        "        \"where're\":\"where are\",\n",
        "        \"where's\":\"where is\",\n",
        "        \"where've\":\"where have\",\n",
        "        \"which's\":\"which is\",\n",
        "        \"who'd\":\"who would\",\n",
        "        \"who'll\":\"who will\",\n",
        "        \"who're\":\"who are\",\n",
        "        \"who's\":\"who is\",\n",
        "        \"who've\":\"who have\",\n",
        "        \"why'd\":\"why did\",\n",
        "        \"why're\":\"why are\",\n",
        "        \"why's\":\"why is\",\n",
        "        \"won't\":\"will not\",\n",
        "        \"wouldn't\":\"would not\",\n",
        "        \"would've\":\"would have\",\n",
        "        \"y'all\":\"you all\",\n",
        "        \"you'd\":\"you would\",\n",
        "        \"you'll\":\"you will\",\n",
        "        \"you're\":\"you are\",\n",
        "        \"you've\":\"you have\",\n",
        "        }\n",
        " \n",
        "def strip_accents(text):\n",
        "    if 'ø' in text or  'Ø' in text:\n",
        "        #Do nothing when finding ø \n",
        "        return text   \n",
        "    text = text.encode('ascii', 'ignore')\n",
        "    text = text.decode(\"utf-8\")\n",
        "    return str(text)\n",
        " \n",
        "def text_cleaning_for_analysis(text):    \n",
        " \n",
        "    #Special case not handled previously.\n",
        "    text = text.replace('\\x92',\"'\")\n",
        "\n",
        "    #Removal of address\n",
        "    text = ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
        " \n",
        "    #Removal of Punctuation\n",
        "    # text = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=\\(\\)]\", \" \", text).split())\n",
        "    text = re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=\\(\\)]\", \"\", text)\n",
        " \n",
        "    #Lower case\n",
        "    text = text.lower()\n",
        " \n",
        "    #CONTRACTIONS source: https://en.wikipedia.org/wiki/Contraction_%28grammar%29\n",
        "    CONTRACTIONS = load_dict_contractions()\n",
        " \n",
        "    text = text.replace(\"’\",\"'\")\n",
        "    words = text.split()\n",
        "    reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words]\n",
        "    text = \" \".join(reformed)\n",
        "\n",
        "    # removing overused words\n",
        "    text = re.sub('reuters|washington|21st century wire says', '', text)\n",
        "\n",
        "    # Standardizing words\n",
        "    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
        " \n",
        "    # Strip accents\n",
        "    text = strip_accents(text)\n",
        "    text = text.replace(\":\",\" \")\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    # remove double spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    \n",
        "    # DO NOT REMOVE STOP WORDS FOR SENTIMENT ANALYSIS - OR AT LEAST NOT NEGATIVE ONES\n",
        " \n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WVTKkdNOFxu"
      },
      "source": [
        "def transform_instance(label, text):\n",
        "  current = []\n",
        "  labelText = \"__label__\" + label.lower().split('/')[-1].split('.')[0]\n",
        "  current.append(labelText)\n",
        "  current.extend(fasttext.tokenize(text_cleaning_for_analysis(text)))\n",
        "  return current"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd3r-3AWXONn"
      },
      "source": [
        "import csv\n",
        " \n",
        "def preprocess(input_files, output_file, keep=1):\n",
        "    i=0\n",
        "    with open(output_file, 'w') as csvoutfile:\n",
        "      csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
        "      for input_file in input_files:\n",
        "        with open(input_file, 'r', newline='') as csvinfile: #,encoding='latin1'\n",
        "          csv_reader = csv.reader(csvinfile, delimiter=',', quotechar='\"')\n",
        "          for row in csv_reader:\n",
        "            if row[1] != 'text' and row[1] != '':\n",
        "              row_output = transform_instance(input_file, row[1])\n",
        "              csv_writer.writerow(row_output)\n",
        "            i += 1\n",
        "            if i%6000 == 0:\n",
        "              print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3sNtK60XUAB"
      },
      "source": [
        "## dataset upsampling and allocating function\n",
        "*upsampling from Charles Malafosse's code*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SECx_dHvXiJa"
      },
      "source": [
        "def upsamplingAllocating(input_file, output_file, ratio_upsampling=1, validation_count=1000):\n",
        "    # allocate data points for validation\n",
        "    # Create files with equal number of tweets for each label\n",
        "    \n",
        "    i=0\n",
        "    counts = {}\n",
        "    dict_data_by_label = {}\n",
        "\n",
        "    # GET LABEL LIST AND GET DATA PER LABEL\n",
        "    with open(input_file, 'r', newline='') as csvinfile: \n",
        "        csv_reader = csv.reader(csvinfile, delimiter=',', quotechar='\"')\n",
        "        for row in csv_reader:\n",
        "            counts[row[0].split()[0]] = counts.get(row[0].split()[0], 0) + 1\n",
        "            if not row[0].split()[0] in dict_data_by_label:\n",
        "                dict_data_by_label[row[0].split()[0]]=[row[0]]\n",
        "            else:\n",
        "                dict_data_by_label[row[0].split()[0]].append(row[0])\n",
        "            i=i+1\n",
        "            if i%10000 ==0:\n",
        "                print(\"read\" + str(i))\n",
        "\n",
        "    # FIND MAJORITY CLASS\n",
        "    majority_class=\"\"\n",
        "    count_majority_class=0\n",
        "    for item in dict_data_by_label:\n",
        "        if len(dict_data_by_label[item])>count_majority_class:\n",
        "            majority_class= item\n",
        "            count_majority_class=len(dict_data_by_label[item])\n",
        "\n",
        "    # allocate validation\n",
        "    validation_per_label = validation_count//len(counts)\n",
        "    val_data=[]\n",
        "    for item in dict_data_by_label:\n",
        "      val_data.extend(dict_data_by_label[item][:validation_per_label])\n",
        "      dict_data_by_label[item] = dict_data_by_label[item][validation_per_label:]\n",
        "      print(f\"{item}: {len(dict_data_by_label[item])}\")\n",
        "      print(dict_data_by_label[item][4000])\n",
        "    \n",
        "    count_majority_class -= 500\n",
        "    \n",
        "    # UPSAMPLE MINORITY CLASS (and split data)\n",
        "    data_upsampled=[]\n",
        "    for item in dict_data_by_label:\n",
        "        data_upsampled.extend(dict_data_by_label[item])\n",
        "        if item != majority_class:\n",
        "            items_added=0\n",
        "            items_to_add = count_majority_class - len(dict_data_by_label[item])\n",
        "            # print(items_to_add)\n",
        "            while items_added<items_to_add:\n",
        "                data_upsampled.extend(dict_data_by_label[item][:max(0,min(items_to_add-items_added,len(dict_data_by_label[item])))])\n",
        "\n",
        "                #print(max(0,min(items_to_add-items_added,len(dict_data_by_label[item]))))\n",
        "\n",
        "                items_added += max(0,min(items_to_add-items_added,len(dict_data_by_label[item])))\n",
        "\n",
        "    # write validation\n",
        "    i = 0\n",
        "    with open(output_file.with_suffix(output_file.suffix + \".validation\"), 'w') as txtoutfile:\n",
        "      for row in val_data:\n",
        "        txtoutfile.write(row+'\\n')\n",
        "        i=i+1\n",
        "        # if i%500 == 0:\n",
        "          # print (\"validation\" + str(i))\n",
        "    \n",
        "    # WRITE TRAIN\n",
        "    i=0\n",
        "    print(len(data_upsampled))\n",
        "    with open(output_file.with_suffix(output_file.suffix + \".train\"), 'w') as txtoutfile:\n",
        "        for row in data_upsampled:\n",
        "            txtoutfile.write(row+ '\\n' )\n",
        "            i=i+1\n",
        "            if i%10000 ==0:\n",
        "                print(\"train\" + str(i))\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4wdnTWmOfWZ"
      },
      "source": [
        "## preprocessing datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvBBYgGsMGeE",
        "outputId": "cbf9ec56-f0d2-445b-b32e-f2dcc8c9a027"
      },
      "source": [
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "data_folder = Path(\"datasets\")\n",
        "datasets_glob = str(data_folder / \"*.csv\")\n",
        "\n",
        "preprocess(glob.glob(str(datasets_glob)), data_folder / \"news.data\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6000\n",
            "12000\n",
            "18000\n",
            "24000\n",
            "30000\n",
            "36000\n",
            "42000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZLDOpR3Y4_k",
        "outputId": "593f11da-2f09-4cec-e851-39a9b7b590dd"
      },
      "source": [
        "upsamplingAllocating(data_folder / \"news.data\", data_folder / \"news\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "read10000\n",
            "read20000\n",
            "read30000\n",
            "read40000\n",
            "__label__true: 20917\n",
            "__label__true the leader of a group of us house republican conservatives said on monday he expects to see text of a revamped bill to repeal and replace obamacare within 24 hours we're waiting to see what the legislative text actually outlines but we remain open minded and willing to look at the details of the plan representative mark meadows chairman of the house of representatives freedom caucus told reporters after a meeting of the group which helped kill a white housebacked plan last month we're hopeful that we will get the legislative text within the next 24 hours\n",
            "__label__fake: 22981\n",
            "__label__fake a louisiana deputy city marshal is in hot water after body cam footage revealed him shooting at a suspect who had his hands up in the air this is sadly becoming too common a story in the news as awareness of problems with officers who have hairtrigger fingers grows what makes this story sadly and horrifically unique though is that deputy derrick stafford also shot and killed the suspect s sixyear old sonand stafford is claiming selfdefense for that because of course he is stafford out of fear for his life and that of his fellow officers began shooting at the vehicle to prevent any further actions by [christopher] few which would put the officers in imminent danger officers went after christopher few after watching an argument between him and his girlfriend outside a bar just before the 2015 shooting they claimed witnessing domestic abuse as the reason they decided to chase down few whose sixyear old son jeremy was in the front passenger seat18 rounds were fired although few had his hands up but no firearm was found in the car and few and jeremy were both shot while still in the car stafford and his partner norris greenhouse said that after few drove forward and then backed up they were in fear for their lives so they firedthat s always the story though it makes us wonder just what police departments are teaching that so many officers shoot first and ask questions later claiming they were in fear for their lives when it s obvious that their suspects were unarmed and even had their hands in the airreports claim that neither stafford nor greenhouse knew jeremy was in the car when they opened fire few was critically wounded and jeremy who was autistic nonverbal and in a specialneeds firstgrade class was murderedstafford s plea of selfdefense is a huge middle finger to few and the rest of jeremy s grieving family arguments will be heard in court next week where judge william bennett will decide whether the indictments standfeatured image via louisiana state police facebook page\n",
            "45962\n",
            "train10000\n",
            "train20000\n",
            "train30000\n",
            "train40000\n",
            "45962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye4Y5fnGtEI7"
      },
      "source": [
        "## training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugiyX4vctH7Q"
      },
      "source": [
        "import datetime\n",
        "import fasttext\n",
        "import os\n",
        " \n",
        "def train(training_data_path, validation_data_path):\n",
        "    print('Training start')\n",
        "    try:                     \n",
        "        print(str(datetime.datetime.now()) + ' START=>')\n",
        " \n",
        "        # Train the model.\n",
        "        model = fasttext.train_supervised(input=training_data_path, autotuneValidationFile=training_data_path, autotuneDuration=6000, autotuneModelSize=\"2M\")\n",
        " \n",
        "        # CHECK PERFORMANCE\n",
        "        print(str(datetime.datetime.now()) + 'Training complete.')\n",
        "        \n",
        "        result = model.test(training_data_path)\n",
        "        validation = model.test(validation_data_path)\n",
        "        \n",
        "        # DISPLAY ACCURACY OF TRAINED MODEL\n",
        "        text_line = \"accuracy:\" + str(result[1:])  + \", validation:\" + str(validation[1:]) + '\\n' \n",
        "        print(text_line)\n",
        "        \n",
        "        # quantize a model to reduce the memory usage\n",
        "        print(\"Model is quantized!\")\n",
        "        return model               \n",
        "        \n",
        "    except Exception as e:\n",
        "        print('Exception during training: ' + str(e) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHpMWzrjtMu1",
        "outputId": "0f7eb1c9-38bb-4d5a-ee04-4889f0f0b34a"
      },
      "source": [
        "# Train your model.\n",
        "model = train(str(data_folder / \"news.train\"), str(data_folder / \"news.validation\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training start\n",
            "2021-01-05 02:56:47.397607 START=>\n",
            "2021-01-05 05:13:52.172013Training complete.\n",
            "accuracy:(0.9871850659240242, 0.9871850659240242), validation:(0.999, 0.999)\n",
            "\n",
            "Model is quantized!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7MGpvB1x5tr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "484ecbd1-6bd7-44dc-c68c-38dc565bf2c3"
      },
      "source": [
        "print(data_folder / \"news.train\")\n",
        "with open(data_folder / \"news.train\", 'r') as txtoutfile:\n",
        "  lines = txtoutfile.read().split(\"\\n\")\n",
        "  print(lines[3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datasets/news.train\n",
            "__label__true us budget chief mick mulvaney on tuesday told staff at the consumer financial protection bureau to disregard instructions from leandra english the deputy director according to a memo consistent with my email from yesterday please disregard any email sent by or instructions you receive from ms english when she is purporting to act as the acting director mulvaney wrote in an email to staff tuesday morning mulvaney and english the agency's deputy director are in a legal fight over who should control the agency following the friday resignation of director richard cordray\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2D2_RqDDdrFO"
      },
      "source": [
        "models_folder = Path(\"drive/MyDrive/fake news models/\")\n",
        "models_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model.save_model(str(models_folder / \"news-classifier-09.ftz\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}